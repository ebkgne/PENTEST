import re,os

from http_request import HTTP_Request

temp_loc = "temp"

DISCOVERY_LIST = ["", "index.asp","index.php","index.aspx","index.html","index.js","index.do","index.cgi","index","acceuil.asp","acceuil.php","acceuil.aspx","acceuil.html","acceuil.js","acceuil.do","acceuil.cgi","acceuil","home.asp","home.php","home.aspx","home.html","home.js","home.do","home.cgi","home","default.asp","default.php","default.aspx","default.html","default.js","default.do","default.cgi","default"]


cells_length = []
cells_string = ""
cells_break = ""
def cell_spacer(string,width,spacer=" "):

    string = str(string)[:width-len(re.findall(r"[\u1100-\u115F\u2E80-\uA4CF\uAC00-\uD7A3\uF900-\uFAFF\uFE10-\uFE1F\uFE30-\uFE6F\uFF00-\uFF60\uFFE0-\uFFE6\U0001F000-\U0001F9FF]", string))]
    for i in range(width+1-len(string)): string+=spacer
    return ""+string

def cells_declare(*args):
    global cells_break
    for arg in args:
        cells_length.append(arg)
    for cell in cells_length: cells_break += cell_spacer("",cell,"-")
    return

cell_pos = 0
def cell_next(content):
    global cell_pos,cells_string
    if not cell_pos < len(cells_length): return
    cells_string += cell_spacer(str(content),cells_length[cell_pos])
    cell_pos += 1
    
def cells_print():
    global cell_pos,cells_string
    print(cells_string)
    cells_string = ""
    cell_pos = 0
    
def cell_full(content):
    x = 0
    for c in cells_length: x += c
    cell_spacer(content,x)
    cells_print()

                    
cells_declare(55,4,7,30,15,12)#,32)


old_parsed_urls= []


def parse_urls(src,domain=""):

        global old_parsed_urls
        new_parsed_urls = []
        
        # Parse Urls
        for x in re.findall(r"(((https?://([\w\d\-]+\.)+\w{2,5}/)?([\w]+[/\\])*[\w\s_\-]+\.(php|aspx|asp|cgi)([\?\#][^(\s|<|'|\"|>)]+)?))",src,re.I|re.DOTALL): 
            new_parsed_urls.append(x[1].strip())
        for x in re.findall(r"<\s*form[^\>]+action=['\"]?([^(\s|'|\")]+)[^>]+>(.+?)<\s*/\s*form\s*[^>]*>",src,re.I|re.DOTALL): new_parsed_urls.append(x[0].strip()+" POST:"+"=xx&".join(re.findall(r"name=[\"']?([^(\s|\"|')]+)",x[1]))+"=xx")
        for x in re.findall(r"(href|link|src)=['\"]?([^(\s|'|\")]+)",src,re.I|re.DOTALL): 
             if not re.findall("^\w+:",x[1]): new_parsed_urls.append(x[1])
        
        updated_new_parsed_urls = []
        
        for i in range(len(new_parsed_urls)): 
      
            url = new_parsed_urls[i]
            
            if len(url) < 3: continue
            
            # Remove some useless
            if re.search(r"wp.content.uploads|xmlrpc|Apache",url): continue 
            
            # Get clean path
            if len(domain): url = re.sub(domain,"",url)
            
            url = re.sub(r"^\.?/","",url)
            url = re.sub(r";$","",url,re.M)
            # url = re.sub(r"(\?.+|;)$","",url,re.M)
            url = url.replace("\\","/")
        
            # Remove useless extensions
            extension = os.path.splitext(url)[1][1:].lower().replace(r"\d","")
            if extension in ["js","html", "htm", "woff","woff2","ico","css", "png", "jpg", "jpeg", "gif", "svg", "ttf", "webp", "pdf", "dtd", "webmanifest", "pro"]: continue
 
            # Check for duplicates before appending
            if url in updated_new_parsed_urls: continue
            if url in old_parsed_urls: continue
            old_parsed_urls.append(url)
            updated_new_parsed_urls.append(url)
            
        return sorted(set((updated_new_parsed_urls)))


requ = HTTP_Request()
do_parse_urls = False
reg_extract = ""
def default_callback(t,r):

    print(cells_break)
    
    extracted=[""]
    if len(reg_extract):
        reg=re.findall(reg_extract,r.body)
        if len(reg): extracted = reg
                    
    cell_next(r.location+r.path)
    cell_next(len(t.threads_queue))
    cell_next(t.threads_count)
    cell_next(r.title)
    cell_next(r.ip)
    cell_next(r.extra["payload"])
    cell_next(extracted[0])
    cells_print()
    
    if do_parse_urls: parse_urls(r.body, r.location)



# p = parse_urls(yolo, "https://moya.evolucare.com/")

# for x in p : print(x)

# exit()
     
if __name__ == '__main__':

    requ.callback = default_callback

    print(cells_break)

    cell_next("URL")
    cell_next("CODE")
    cell_next("LENGTH")
    cell_next("TITLE")
    cell_next("IP")
    cell_next("PAYLOAD")
    cell_next("EXTRACT")
    
                    
    import argparse
    parser = argparse.ArgumentParser()

    parser.add_argument('url', help='target URL')
    parser.add_argument('-d', '--data', help='POST data')
    parser.add_argument('-c', '--cookies', help='Cookies')
    parser.add_argument('-f', '--follow', help='follow')
    parser.add_argument('-i', '--inputfile', help='input file')
    parser.add_argument('-p', '--payload', help='Paylaod')
    parser.add_argument('-t', '--threads', help='Thread MAX')
    parser.add_argument('-o', '--output', help='Output File')
    parser.add_argument('-e', '--extract', help='Regex to extract Data')
    parser.add_argument('-4xx', '--no4xx', action='store_true', help='No 4xx')
    parser.add_argument('-D', '--discover', action='store_true', help='Auto Mode')
    parser.add_argument('--debug', action='store_true', help='Print out CURL CMD')
    parser.add_argument('--parse', action='store_true', help='Parse URLS')
    parser.add_argument('--src', action='store_true', help='show src')
    parser.add_argument('--sess', action='store_true', help='get session')

    args = parser.parse_args()
    
    if args.debug: DEBUG = True

    requ.set_url(args.url)

    requ.callback = default_callback
    
    if args.cookies: requ.cookies_str = args.cookies
    
    if args.sess: requ.get_cookies()
    
    if args.follow: requ.follow = int(args.follow)
    
    # if args.output: requ.tofile(args.output)
    
    if args.data: requ.post_str = args.data
    
    if args.extract: reg_extract = args.extract

    if args.threads: requ.MAX_REQUEST = int(args.threads)
    
    if args.parse: do_parse_urls = True
        
    if args.no4xx: requ.rules = (range(400,410  ))
  
    if args.discover:  
        
        requ.set_unknow_page()
        
        do_parse_urls = True
        
        requ.set_url(requ.url_location+"/..1..")
        
        # Parse for sql warnings
        requ.extract_reg = "(.+(sql|SQL)[^(\w|\d)].+)"
        
        # # Get robots.txt
        cell_full("# ROBOTS.TXT")
        cell_full("")
        robots_txt = requ.query("robots.txt")

        # if robots_txt.code == 200: print("\n"+robots_txt.body+"\n")
        
        # Find techs
        cell_full("# DISCOVERY")
        cell_full("")
        x = requ.from_list(DISCOVERY_LIST)
        
        # fuzz
        cell_full("# FUZZ")
        cell_full("")
        requ.set_url(requ.url_location+"/..1../")
        requ.from_file("../lists/fuzz.txt")
        
        # requ.url_str = requ.url_location+"/..1...php"
        # requ.from_file("../../lists/fuzz.txt")
        
        exit()

    if args.inputfile: 
        requ.from_file(args.inputfile)
        exit()
    
    payload=""
    if args.payload: payload = args.payload
    x  = requ.query(args.payload)
    
    if len(old_parsed_urls): 
        
        requ.set_url(requ.url_location+"/..1..")
        
        requ.post_str = ""
        
        requ.follow = 1
        
        for url in old_parsed_urls:
                        
            if re.match(r"^https?://",url): continue
                # url = r.location+"/"+url
        
        cell_full("   "+(url)[-173:])
    
        requ.from_list(old_parsed_urls)
# print ("FIN")
    print("+---------------------------------------------------------+------+---------+--------------------------------+-----------------+--------------+----------------------------------+")
    if args.src: 
        print()
        print(x.requ_header)
        print()
        print(x.resp_header)
        print()
        print(x.body)
    print("\n")
    