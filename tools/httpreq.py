import subprocess,re, keyboard, threading, urllib.request,os, chardet, datetime, random, ast

from datetime import datetime
from urllib.parse import urlparse
path_to_logs = "C:/Users/tourkit/Desktop/netty/logs/"

temp_loc = "kobeyes/"

def urlencode(string):
    return urllib.parse.quote(string)

def get_domain(url):
    return ".".join(urlparse(url).netloc.split(".")[-2:])

class HTTP_Response:
    
    raw = ""
    body = ""
    req_header = ""
    header = ""
    location = ""
    payload = []
    code = 0
    length = 0
    extract = []
    title = ""
    
    def learn(t):

        code = re.search(r"HTTP[^\s]+\s+(\d{3})", t.header)
        if code: t.code = int(code.group(1))

        length = re.search(re.compile(r"Content.Length. (\d+)", re.DOTALL), t.header)
        if length: t.length = int(length.group(1))     

        if not len(t.location): t.location = t.req_header.splitlines()[0]
        
        body_title = re.search(r"title\s*>\s*([^<]+)\s*<",t.body)
        if body_title: t.title = body_title[1]
        
        t.payload = ast.literal_eval(t.body.splitlines()[-1].split(":")[1])
    
        t.body = "\n".join(t.body.splitlines()[:-1])
        t.length = len(t.body)
        # t.body.splitlines()#[-1]
        
    def __str__(t):
        
        maxlen = 1000
        
        output = "####### HTTP_Response ###########\r\n\r\n"
        output += "LOCATION: "+t.location + "\r\n"
        output += "PAYLOAD: "+t.payload + "\r\n"
        output += "HTTP CODE: "+str(t.code) + "\r\n"
        output += "LENGTH: "+str(t.length) + "\r\n"
        output += "TITLE: "+t.title + "\r\n\r\n"
        output +="========= req_header ===========\r\n"
        output += t.req_header[:maxlen] + "\r\n\r\n"
        output +="========= header ===========\r\n"
        output += t.header[:maxlen] + "\r\n\r\n"
        output +="========= body ===========\r\n"
        output += t.body[:maxlen] + "\r\n\r\n"
        output +="========= extract ===========\r\n"
        for e in t.extract[:(round(maxlen/10))]:
            output += " - " + e + "\r\n"
            
        output +="\r\n"
        output +="####### END ###########"
        
        return output
        
    def store(t):
        
        path = t.location.split("://")[1]
        
        path = list(re.findall(r"^(.+?)(\?.+)?$", path)[0])
        if path[0][-1] != "/": path[0] += "/"
        if len(path[1]): path[1]  = urllib.parse.quote(path[1])+"_"
        path[1] += datetime.datetime.now().strftime('%y%m%d_%H%M%S')+"_"+str(random.randint(10000000, 99999999))
        path = "crawls/"+temp_loc+"".join(path)+".html"
        
        directory = os.path.dirname(path)
        if not os.path.exists(directory): os.makedirs(directory)
            
        to_write = ""
        to_write = "<!--"
        to_write += "\r\n=== REQUEST HEADER =======================================\r\n"
        to_write += t.req_header
        to_write += "\r\n=== RESPONSE HEADER =======================================\r\n"
        to_write += t.header
        to_write += "\r\n-->\r\n"
        to_write += t.body
        
        with open(path, 'w', encoding='utf-8') as file: file.write(to_write)   
               
        return path
        
            
class HTTP_Request:
    
    ## QUERY BUILDER 
    
    reg_pattern = ""
    followredir = ""
    flags_list = []
    useragent = []
    referer = []
    domain = []
    path = []
    get_data = []
    post_data = []
    auto_referer = True
    query_count = 0
  
    MAX_REQUEST = 10
    MAX_CHANGED = False
    domain_only = ""
    parsed_urls = []
       
    callback = lambda t,r: print(r.location+" "+str(r.code)+":"+str(r.length)+":"+r.title)
    
    payloads = []
    
    def __init__(t, url = "", post="", flags_list= "w \"\\n..COUNT..:..PAYLAOD..\"" ):
        
        if len(url): t.url(url)
        if len(post): t.post(post)
        if len(flags_list): t.flags(flags_list)
 
        t.ua('Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36')

        print("TARGET: "+t.payload_join(t.domain, "[PAYLOAD]"))

    def query_split(t, q):
        parts = re.split(r'\.\..+?\.\.', q)
        separators = re.findall(r'\.\.(.+?)\.\.', q)
        result = []
        for i, part in enumerate(parts):
            result.append(part)
            if i < len(separators):
                result.append(separators[i])
        return result

    def url(t, url):
            
        results = re.findall(r"^((\.\.([^(\.\.)]+)\.\.)?http.?.//[^/]+)?(/?((.*)/)*(.*))?$",url, re.I)[0]
        if len(results)>0: t.domain = t.query_split(results[0]+"/")
        if len(results)>4: t.path =  t.query_split(results[4])
        if len(results)>6: t.get(results[6])
        t.domain_only = get_domain(results[0])
    
    def get(t, data): t.get_data =  t.query_split(data)     

    def post(t, data): t.post_data =  t.query_split(data) 
    
    def ua(t, data): t.useragent =  t.query_split(data) 
    
    def ref(t, data): t.referer =  t.query_split(data) 
    
    def flags(t, data): t.flags_list =  t.query_split(data) 

    def follow(t, maxredir=1):
       if maxredir: t.followredir = "  -L --max-redirs "+str(maxredir)
       else: t.followredir =""
    
    def extract(t, pattern): 
        t.reg_pattern = pattern
        t.callback = lambda r: print("\n".join(r.extract))
        
    def payload_join(t, lst, payload):
        out_str = ""
        lst_len = len(lst)
        
        if lst_len: 
            if len(lst[0]): out_str+=lst[0]
            
            if lst_len>1:
                if isinstance(payload, str): payload = [payload]
                        
                for i in range(int((lst_len-1)/2)):
                    i2 = i*2+1; name = lst[i2]; i2+=1
                    
                    if name == "COUNT": 
                        out_str += str(t.query_count) + lst[i2]
                        continue
                    if name == "PAYLAOD": 
                        out_str += urllib.parse.unquote(str(payload)) + lst[i2]
                        continue

                    if isinstance(payload, list): 
                        name = min(int(name),len(name)-1) if name.isdigit() else 0
                        out_str += payload[name]
                        out_str +=  lst[i2]
                        continue
               
                    
                    if name in payload: out_str += payload[name] 
                    else: out_str += ".."+name+".."
                    
                    out_str += lst[i2]
                    
        return out_str
    
    def query(t, payload, save_file=""):
        
        t.payloads = [payload]
        start_time = datetime.now()
        
        t.query_count+=1
        
        # print("DEBUG PAYLOAD: " + str(payload)) 
        
        domain = t.payload_join(t.domain,payload)
        
        command = "curl --retry 10 --retry-connrefused -v -s -sS" + t.followredir
        
        if len(t.flags_list): command += ' '+t.payload_join(t.flags_list,payload)
        
        if len(t.post_data): 
            
            command += ' -X POST -d "'+t.payload_join(t.post_data,payload)+'"'
        
            # for big POST datas use   following:
            # with open(path_to_logs+"temp_data.txt", 'w') as file: file.write(t.payload_join(t.post_data,payload))  
            # command += ' -X POST -d @'+path_to_logs+"temp_data.txt"

        if len(t.useragent): command += ' -H "User-Agent:'+t.payload_join(t.useragent,payload)+'"'
        
        if t.auto_referer: command += ' -e "'+domain+'"'
        else:
            if len(t.referer): command += ' -e "'+t.payload_join(t.referer,payload)+'"'
        full_domain = ""   
        full_domain += domain
        full_domain += t.payload_join(t.path,payload)
        full_domain += t.payload_join(t.get_data,payload)
        
        command += ' "'
        command += full_domain
        command += '"'
        
        results = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)
        # print("DEBUG COMMAND: " + str(command)) 
        # print("DEBUG"); exit(results)
    
        end_time = datetime.now()
        time_difference = (end_time - start_time).total_seconds()
        
        # beyond 5s response time: divide max reqs
        if (time_difference>5 and t.MAX_CHANGED): 
            t.MAX_CHANGED = False
            t.MAX_REQUEST = int(t.MAX_REQUEST*.5)
            print("MAX_REQUEST: "+str(t.MAX_REQUEST))

        
        response = HTTP_Response()
        detected_encoding = chardet.detect(results.stdout)['encoding']
        
        response.body = ""
        if  detected_encoding: 
            try:
                response.body = results.stdout.decode(detected_encoding)
            except UnicodeDecodeError:
                response.body = results.stdout.decode('utf-8', 'ignore')
        
        detected_encoding = chardet.detect(results.stderr)['encoding']
        rerr = full_domain+"\r\n"+re.sub(r"[<>]","",results.stderr.decode(detected_encoding))    
        
        if re.search(r"Could not resolve", rerr): 
            response.req_header = full_domain
            response.header = ""
            return 
      
        response.req_header = rerr
        failed = re.search(r"curl: \([^\)]+\) Failed to connect(.+)\n", response.req_header)
        if not failed: 
            header_split = response.req_header.split("\r\r\n \r\r\n")
            response.req_header = re.sub("[\r\n]+","\n",header_split[0])
            if len(header_split)>1: response.header = re.sub("[\r\n]+","\n",header_split[-2])
        response.learn()
        
        # if response.code and response.code >299 and response.code <399:  print("url_should_be")
        
        if response.code and response.code == 404: return
        
        # parse HTML for more urls # could add support to parse get params, also parse urls in js code
        for f in re.findall(r"(link|href|src)\s*=\s*[\"'](\./)?([^(\"')]*)[\"']",response.body,re.I):    
            
            f = f[2]
            fdomain = get_domain(f)
                    
            if len(fdomain) and t.domain_only != fdomain: continue
            
            path = urlparse(f).path
            if len(path) and path[0] == "/": path = path[1:]

            ext = path.split(".")[-1]
            if ext in ["jpg","css","png","js", "gif","html", "woff", "woff2", "svg", "eot", "ttf"]: continue
            
            if not re.match(r"^http",f): f = t.payload_join(t.domain,"")+f
            parsed_url = urlparse(f)
            f = parsed_url.scheme+"://"+str(parsed_url.netloc+"/"+parsed_url.path).replace("//","/").replace("//","/").replace("../","")

            if not f==response.location and not f in t.parsed_urls: 
                print(f)
                t.parsed_urls.append(f)
        
        # print (response.body)
        
        if len(t.reg_pattern): response.extract = re.findall(r'<h2 style="text-align: center;" class="page-title">(.+)', response.body.replace(r"\s+"," "), re.MULTILINE)
        # return response.extract
        if len(save_file): 
            to_print = ""
            if len(response.extract): to_print = ("\n".join(response.extract))
            else: to_print = str(response)
            with open(save_file, 'w', encoding='utf-8') as file: file.write(to_print)   
        # print("===========================================\n"+response.body+"\n")
        with open("C://REPOSITORY/WEB/burp/netty/httpreq_last.html", 'w', encoding='utf-8') as file: file.write(("response.body"))   
        return t.callback(response)
  
    threads = [] 
    pending = []

    def on_key_pressed(t, event): 
        # return
        
        if len(t.payloads) and event.scan_code == 44: print("POSITION:w "+" : ".join(t.payloads)+ "")
        # print("keycode:"+str(event.scan_code))

    def add_thread(t, payload):
        
        thread = threading.Thread(target=t.query, args=([payload]))
        thread.start()
        t.threads.append(thread)  
    
        # remove thread from threads when http_req came back
    
    def from_list(t, lst):
        
        lst = list(set([str(item).strip() for item in lst]))
        
        i = 0
        t.threads = []
        for p in lst: 
            
            t.add_thread(p)
            
            if i%t.MAX_REQUEST == t.MAX_REQUEST-1 or (len(t.pending)-i<t.MAX_REQUEST and i%t.MAX_REQUEST == (len(t.pending)%t.MAX_REQUEST-1)):    
                
                try: 
                    t.MAX_CHANGED = True
                    for thread in t.threads: thread.join()
                    
                except KeyboardInterrupt: os._exit(1)
                
                t.threads.clear()
                
            i+=1
        
    def from_file(t, file):
        keyboard.on_press(t.on_key_pressed)
        i = 0
    
        with open(file, 'r') as file: 
            for line in file: 
                line = line.strip()
                t.pending.append((line))
                # t.pending.append(urllib.parse.quote(line))
                i+=1
                
        t.from_list(t.pending)


# fait init si 302 nanin

# r = HTTP_Request("https://support.studiosport.fr")
# r.from_list(range(2))

# for i in r.parsed_urls: print(i)

# exit()
if __name__ == '__main__':
                   
    import argparse
    parser = argparse.ArgumentParser()

    parser.add_argument('url', help='target URL')
    parser.add_argument('-p', '--post', help='POST data')
    parser.add_argument('-f', '--follow', help='follow')
    parser.add_argument('-i', '--inputfile', help='input file')
    parser.add_argument('-pl', '--payload', help='Paylaod')
    parser.add_argument('-o', '--output', help='Output File')

    args = parser.parse_args()
    url = args.url
    payload = args.payload
    output = args.output

    t_url = url

    r = HTTP_Request(url)

    if args.follow: r.follow(args.follow)
    if args.post: r.post(args.post)

    if not args.payload: args.payload = "" 

    if args.output: r.query(args.payload, args.output)
    else: 
        if args.inputfile:
            r.from_file(args.inputfile) 
        else:
            r.query(args.payload)

    