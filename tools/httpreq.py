import subprocess,re, keyboard, threading, urllib.request,os, chardet, datetime, random, ast,json,math

from datetime import datetime
from urllib.parse import urlparse
path_to_logs = "C:/Users/tourkit/Desktop/netty/logs/"

temp_loc = "kobeyes/"

RAMDOM_VALUE = "RAMDOM_VALUE"

DISCOVERY_LIST = ["index.asp","index.php","index.aspx","index.html","index.js","index.do","index.cgi","index","acceuil.asp","acceuil.php","acceuil.aspx","acceuil.html","acceuil.js","acceuil.do","acceuil.cgi","acceuil","home.asp","home.php","home.aspx","home.html","home.js","home.do","home.cgi","home","default.asp","default.php","default.aspx","default.html","default.js","default.do","default.cgi","default"]

DEBUG = False

def urlencode(string):
    return urllib.parse.quote(string)

def get_domain(url):
    return ".".join(urlparse(url).netloc.split(".")[-2:])

def spacer(string,width=20):
    string = str(string)[:width]
    for i in range(width+1-len(string)): string+=" "
    return "| "+string

def payloadify(string,payloads=[]):
    
    if type(payloads) != list: payloads = [str(payloads)]
    for x in re.findall(r"\.\.(\d+)\.\.",string): string = string.replace(".."+x+"..",payloads[int(x)-1])
    return string 




class HTTP_Packet:
   
    body = ""
    requ_header = ""
    resp_header = ""
    code = 0
    length = 0
    ip = ""
    domain = ""
    location = ""
    path=""
    title = ""
    extracted = [""]
    extra = {}
    
    def __init__(t,results):   
        
        # Read STDOUT
        body = results.stdout
        encoding = chardet.detect(body)['encoding']
        if  encoding: 
            try: body = body.decode(encoding)
            except UnicodeDecodeError: body = body.decode('utf-8', 'ignore')

        body = body.splitlines()
        t.extra = json.loads(body[0])
        t.body = "\n".join(list(body[1:]))
        
        # Read STDERR and check for errors
        rerr = re.sub(r"^[^\*]\s","",results.stderr.decode(chardet.detect(results.stderr)["encoding"]),flags=re.M)
        if re.search(r"Could not resolve", rerr): return 
        failed = re.findall(r"curl: \([^\)]+\) .+\n", rerr)
        if failed: return #print(failed)

        # Parse Response Header
        resp_header = re.findall(r'(HTTP/\d+\.\d+\s\d{3}.+?)\s*\n\s*\n', rerr,flags=re.DOTALL)
        t.resp_header = resp_header[-1]
        
        # HTTP code
        code = re.search(r"HTTP[^\s]+\s+(\d{3})", t.resp_header)
        if code: t.code = int(code.group(1))
        
        # Content lenght
        length = re.search(re.compile(r"Content.Length. (\d+)", re.DOTALL), t.resp_header)
        if length: t.length = int(length.group(1))
        else: t.length = len(t.body)  

        # Page title
        body_title = re.search(r"title\s*>\s*([^<]+)\s*<",t.body, re.I)
        if body_title: t.title = re.sub(r"\s+"," ",body_title[1])
        
        # Parse Response Headers 
        requ_headers = re.findall(r"\* Connected to ([^\s]+) \(([^\)]+)\) port (\d+).+?\n(.+?)\s+\n\s*\n",rerr,flags=re.DOTALL)
        t.requ_header = requ_headers[-1][3]
        
        # Path
        t.path = re.findall(fr"[^\s]+[^\s]+\s([^\s]+)\sHTTP/\d\.\d",t.requ_header,re.M)[0]
        
        # IP
        t.ip = requ_headers[-1][1]
        
        # Domain and Location
        t.domain = requ_headers[-1][0]
        t.location = "http"
        if int(requ_headers[-1][2]) == 443: t.location += "s"
        t.location += "://"+t.domain

    def __str__(t): return spacer(t.location+t.path,55) + spacer(str(t.code),4) + spacer(str(t.length),7) + spacer(t.title,30) + spacer(t.ip,15) + spacer(t.extra["payload"],12)+spacer(t.extracted[0],32)+"|"
        
    def store(t):
        
        path = t.location.split("://")[1]
        
        path = list(re.findall(r"^(.+?)(\?.+)?$", path)[0])
        if path[0][-1] != "/": path[0] += "/"
        if len(path[1]): path[1]  = urllib.parse.quote(path[1])+"_"
        path[1] += datetime.datetime.now().strftime('%y%m%d_%H%M%S')+"_"+str(random.randint(10000000, 99999999))
        path = "crawls/"+temp_loc+"".join(path)+".html"
        
        directory = os.path.dirname(path)
        if not os.path.exists(directory): os.makedirs(directory)
            
        to_write = "<!--"
        to_write += "\r\n=== REQUEST HEADER =======================================\r\n"
        to_write += t.requ_header
        to_write += "\r\n=== RESPONSE HEADER =======================================\r\n"
        to_write += t.resp_header
        to_write += "\r\n-->\r\n"
        to_write += t.body
        
        with open(path, 'w', encoding='utf-8') as file: file.write(to_write)   
               
        return path
    
class HTTP_Request:
    
    # Request data
    useragent_str = ""
    referer_str = ""
    url_str = ""
    post_str = ""
    
    # Rules
    follow = 0
    auto_referer = True
    MAX_REQUEST = 25
    
    # Extract 
    extract_reg = "(.+(sql|SQL)[^(\w|\d)].+)"

    # Curl Config
    flags = ""
  
    rules = []
    
    # Private data
    url_location = ""
    url_path = ""

    
    def __init__( t, url = "", post = "", flags = "-m 4", useragent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36" ):
        
        t.post_str = post
        t.flags = flags
        t.referer_str = useragent
        t.useragent_str = useragent
        
        t.set_url(url)


        print("\nTARGET: "+payloadify(urlparse(url).netloc, "[PAYLOAD]")+"\n")
        print("+---------------------------------------------------------+------+---------+--------------------------------+-----------------+--------------+----------------------------------+")
        print(spacer("URL",55) + spacer("CODE",4) + spacer("LENGTH",7) + spacer("TITLE",30) + spacer("IP",15) + spacer("PAYLOAD",12)+spacer("EXTRACT",32)+"|")
        print("+---------------------------------------------------------+------+---------+--------------------------------+-----------------+--------------+----------------------------------+")


    def set_unknow_page(t):
        
        temp_url = t.url_str
        t.url_str = (t.url_location+"/..1..")
        x = t.query(RAMDOM_VALUE)
        t.default_ultra_random = [x.code, x.length, x.title]
        t.url_str = temp_url

    def set_url(t, url):
        
        t.url_str = url
        
        # Parse URL
        urlparsed = urlparse(url)
        t.url_location = urlparsed.scheme+"://"+urlparsed.netloc
        t.url_path = urlparsed.path 
        t.url_path += ("?"+urlparsed.query if urlparsed.query else "")
        t.url_path += ("#"+urlparsed.fragment if urlparsed.fragment else "")
        
    query_count = 0
    MAX_CHANGED = False
    callback = lambda x,t,r: print(r)
        
    def query(t, payloads):
        
        start_time = datetime.now()
        
        command = "curl -v -s -sS"
        
        if t.follow: command += " -L --max-redirs "+str(t.follow)

        if len(t.useragent_str): command += ' -H "User-Agent:'+payloadify(t.useragent_str,payloads)+'"'
        
        if len(t.referer_str): command += ' -e "'+payloadify(t.referer_str,payloads)+'"'

        if len(t.flags): command += " "+t.flags
        
        command += " \""+ payloadify(t.url_str,payloads)+ "\""
        
        if len(t.post_str): 
            
            command += ' -X POST -d "'+payloadify(t.post_str,payloads)+'"'
        
            # for big POST datas use   following:
            # with open(path_to_logs+"temp_data.txt", 'w') as file: file.write(payloadify(t.post_str,payloads))  
            # command += ' -X POST -d @'+path_to_logs+"temp_data.txt"
        
        if DEBUG: print(command) 
        results = subprocess.run("echo " + json.dumps({ "count":t.query_count, "payload":urllib.parse.unquote(str(payloads)) })+" && " + command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)
        # print("DEBUG"); exit(results)
        
        t.query_count+=1
    
        # beyond 5s response time: divide max reqs
        if ((datetime.now() - start_time).total_seconds()>5 and t.MAX_CHANGED): 
            t.MAX_CHANGED = False
            # t.MAX_REQUEST = math.ceil(t.MAX_REQUEST*.5)
            # print("MAX_REQUEST: "+str(t.MAX_REQUEST))
        
        with t.print_lock:

            r = HTTP_Packet(results)
            # print(r)
            if r.code: # could be removed ?
                if not r.code in t.rules and len(t.extract_reg):
                    c = 0
                    c += t.default_ultra_random[0] == r.code
                    c += t.default_ultra_random[1] == r.length
                    c += t.default_ultra_random[2] == r.title
                    if c < 2: 
                        
                        extracted=re.findall(t.extract_reg,r.body)
                        r.extracted= []
                        for x in extracted: r.extracted.append(x)
                        if not len(extracted): r.extracted= [""]
                        
                        t.callback(t,r)
            return r
  
    threads = [] 
    pending = []

    def on_key_pressed(t, event): 
        return
        
        if len(t.payloads) and event.scan_code == 44: print("POSITION: "+" : ".join(t.payloads)+ "")
        # print("keycode:"+str(event.scan_code))
    
    print_lock = threading.Lock()
    
    default_ultra_random = [0,0,0]
    
    def from_list(t, payloads):
      
        keyboard.on_press(t.on_key_pressed)
        
        if type(payloads) != list: payloads = [payloads]
        payloads = list(set([str(item).strip() for item in payloads]))
        
        t.threads = []
        for i in range(len(payloads)): 
            
            thread = threading.Thread(target=t.query, args=([payloads[i]]))
            thread.start()
            t.threads.append(thread)  
            
            # remove thread from threads when http_req came back
    
            if i%t.MAX_REQUEST == t.MAX_REQUEST-1 or (len(t.pending)-i<t.MAX_REQUEST and i%t.MAX_REQUEST == (len(t.pending)%t.MAX_REQUEST-1)):    
                
                try: 
                    t.MAX_CHANGED = True
                    for thread in t.threads: thread.join()
                    
                except KeyboardInterrupt: os._exit(1)
                
                t.threads.clear()       
            
    def from_file(t, file):
        
        with open(file, 'rb') as file2: enc = chardet.detect(file2.read())["encoding"]

        with open(file, 'r', encoding=enc) as file: 
            
            for line in file: t.pending.append(str(line.strip()))

        return  t.from_list(t.pending)
    


parsed_urls = []
def parse_urls_(reg,o):
    
    global parsed_urls
    for x in re.findall(reg,o, re.I|re.DOTALL) : 
        strip = x[1].strip()
        if x[0] == "action": 
            
            strip = "POST: "+strip+" "+"=xx&".join(re.findall(r"name=[\"']?([^(\s|\"|')]+)",x[2]))+"=xx"
        x = strip
        path = urlparse(x).path
        
        if re.search(r"wp.content.uploads|xmlrpc|Apache",x): return
        
        extension = os.path.splitext(path)[1][1:].lower().replace(r"\d","")
        if not extension in ["js","html", "htm", "woff","woff2","ico","css", "png", "jpg", "jpeg", "gif", "svg", "ttf", "webp", "pdf", "dtd", "webmanifest", "pro", ""]: parsed_urls.append(x)

def parse_urls(o):
    
        global parsed_urls
        parsed_urls= []
        
        # Parse Urls
        parse_urls_(r"(([\w\s_\-]+\.(php|asp|aspx|cgi)([\?\#][^(\s|<|'|\"|>)]+)?))",o)
        parse_urls_(r"(([\w\d_\-]+(/[\w\d_\-\?\#=%&\.]+)+))",o)
        parse_urls_(r"(action)=[\"'](.*?)[\"'](.+)<\s*/\s*form",o)
        parse_urls_(r"(action)=(?!'|\"|\s)+\b",o)
        parse_urls_(r"(href)=[\"'](.*?)[\"']",o)
        parse_urls_(r"(href)=(?!'|\"|\s)+\b",o)
        parse_urls_(r"(src)=[\"'](.*?)[\"']",o)
        parse_urls_(r"(src)=(?!'|\"|\s)+\b",o)
        
        p = []
        for i in range(len(parsed_urls)): 
            t = "/"+re.sub(r"^\.?/","",parsed_urls[i])
            if not t in p: p.append(t)
        
        return p
    

do_parse_urls = False    
def default_callback(t,r):
    
    print(spacer(r.location+r.path,55) + spacer(str(r.code),4) + spacer(str(r.length),7) + spacer(r.title,30) + spacer(r.ip,15) + spacer(r.extra["payload"],12)+spacer(r.extracted[0],32)+"|")
    
    if do_parse_urls:
        for url in parse_urls(r.body) : print(spacer("   "+r.location+url,173)+"|")
    
    print("+---------------------------------------------------------+------+---------+--------------------------------+-----------------+--------------+----------------------------------+")
    
    return    
        
if __name__ == '__main__':
                   
    import argparse
    parser = argparse.ArgumentParser()

    parser.add_argument('url', help='target URL')
    parser.add_argument('-d', '--data', help='POST data')
    parser.add_argument('-f', '--follow', help='follow')
    parser.add_argument('-i', '--inputfile', help='input file')
    parser.add_argument('-p', '--payload', help='Paylaod')
    parser.add_argument('-o', '--output', help='Output File')
    parser.add_argument('-e', '--extract', help='Regex to extract Data')
    parser.add_argument('-4xx', '--no4xx', action='store_true', help='No 4xx')
    parser.add_argument('-D', '--discover', action='store_true', help='Auto Mode')
    parser.add_argument('--debug', action='store_true', help='Print out CURL CMD')
    parser.add_argument('--parse', action='store_true', help='Parse URLS')

    args = parser.parse_args()

    r = HTTP_Request(args.url)
    r.callback = default_callback
    
    # r.query("ooo")
    # exit()

    # if args.output: r.tofile(args.output)
    
    if args.follow: r.follow = int(args.follow)
    
    if args.data: r.post_str = args.data
    if args.extract: r.extract_reg = args.extract
    
    if args.debug: DEBUG = True
    
    if args.parse: do_parse_urls = True
    
    if args.discover:  
        
        r.set_unknow_page()
        
        do_parse_urls = True
        
        r.set_url(r.url_location+"/..1..")
        
        # # Get robots.txt
        print(spacer("# ROBOTS.TXT ",173)+"|\n"+spacer("",173)+"|")
        robots_txt = r.query("robots.txt")
        # if robots_txt.code == 200: print("\n"+robots_txt.body+"\n")
        
        # Find techs
        print(spacer("# DISCOVERY",173)+"|\n"+spacer("",173)+"|")
        r.from_list(DISCOVERY_LIST)
        
        # fuzz
        print(spacer("# FUZZ",173)+"|\n"+spacer("",173)+"|")
        r.set_url(r.url_location+"/..1../")
        r.from_file("../../lists/fuzz.txt")
        
        # r.url_str = r.url_location+"/..1...php"
        # r.from_file("../../lists/fuzz.txt")
        
        # Parse for sql warnings
        # (.+(sql|SQL)[^(\w|\d)].+)
        
        # 
        exit("\n")
        
    if args.no4xx: r.rules = (range(400,410  ))
    
    if not args.payload: args.payload = "" 
    
    if args.inputfile: 
        r.set_unknow_page()
        exit(r.from_file(args.inputfile)) 
    
    x  = r.query(args.payload)
    print("\n")
    
    if not args.extract: print(x.body)
    