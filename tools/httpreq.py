import subprocess,re, keyboard, threading, urllib.request,os, chardet, datetime, random, ast,json,math

from datetime import datetime
from urllib.parse import urlparse

temp_loc = "temp"

RAMDOM_VALUE = "RAMDOM.VALUE"

DISCOVERY_LIST = ["", "index.asp","index.php","index.aspx","index.html","index.js","index.do","index.cgi","index","acceuil.asp","acceuil.php","acceuil.aspx","acceuil.html","acceuil.js","acceuil.do","acceuil.cgi","acceuil","home.asp","home.php","home.aspx","home.html","home.js","home.do","home.cgi","home","default.asp","default.php","default.aspx","default.html","default.js","default.do","default.cgi","default"]

DEFAULT_USERAGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36"
DEFAULT_USERAGENT = "MichMich"

DEBUG = False

def spacer(string,width=20):
    # account for double char space using in the terminal  chars like chineses
    width -= len(re.findall(r"[\u1100-\u115F\u2E80-\uA4CF\uAC00-\uD7A3\uF900-\uFAFF\uFE10-\uFE1F\uFE30-\uFE6F\uFF00-\uFF60\uFFE0-\uFFE6\U0001F000-\U0001F9FF]", string))
    string = str(string)[:width]
    for i in range(width+1-len(string)): string+=" "
    return "| "+string

def payloadify(string,payloads=[]):
    try: payloads = ast.literal_eval(payloads)
    except : payloads = [payloads]
    for x in re.findall(r"\.\.(\d+)\.\.",string): 
        
        i = int(x)-1
        string = string.replace(".."+x+"..",(payloads[i] if i < len(payloads) else "" ))
        
    return string 

class HTTP_Packet:
   
    body = ""
    requ_header = ""
    resp_header = ""
    code = 0
    length = 0
    cookies = ""
    ip = ""
    domain = ""
    location = ""
    path=""
    title = ""
    extra = {}
    
    def __init__(t,results):   
        
        # Read STDOUT
        body = results.stdout
        encoding = chardet.detect(body)['encoding']
        if  encoding: 
            try: body = body.decode(encoding)
            except UnicodeDecodeError: body = body.decode('utf-8', 'ignore')

        body = body.splitlines()
        t.extra = json.loads(body[0])
        t.body = "\n".join(list(body[1:]))
        # print(t.body)
        
        # Read STDERR and check for errors
        rerr = re.sub(r"^[^\*]\s","",results.stderr.decode(chardet.detect(results.stderr)["encoding"]),flags=re.M)
        # print(rerr)
        if re.search(r"Could not resolve", rerr): return 
        failed = re.findall(r"curl: \([^\)]+\) .+\n", rerr)
        if failed: return #print(failed)

        # Parse Response Header
        resp_header = re.findall(r'(HTTP/\d+\.\d+\s\d{3}.+?)\s*\n\s*\n', rerr,flags=re.DOTALL)
        t.resp_header = resp_header[-1]
        
        # HTTP code
        code = re.search(r"HTTP[^\s]+\s+(\d{3})", t.resp_header)
        if code: t.code = int(code.group(1))
        
        # Cookies
        cookies = re.findall(r"Set.Cookies?:\s*([^\n]+)\s*", t.resp_header)
        if cookies: t.cookies = cookies[0].split(";")

        # Content lenght
        length = re.search(re.compile(r"Content.Length. (\d+)", re.DOTALL), t.resp_header)
        if length: t.length = int(length.group(1))
        else: t.length = len(t.body)  

        # Page title
        body_title = re.search(r"title\s*>\s*([^<]+)\s*<",t.body, re.I)
        if body_title: t.title = re.sub(r"\s+"," ",body_title[1])
        
        # Parse request Headers 
        requ_headers = re.findall(r"\* Connected to ([^\s]+) \(([^\)]+)\) port (\d+).+?\n(\*.+?\n)*(.+?)\s+\n\s*\n",rerr,flags=re.DOTALL)
        # for x in requ_headers[0]: print(x[:150]+"\n-----")
        # exit()
        t.requ_header = requ_headers[-1][4]

        # Path
        t.path = re.findall(fr"[^\s]+[^\s]+\s([^\s]+)\sHTTP/\d\.\d",t.requ_header,re.M)[0]
        
        # IP
        t.ip = requ_headers[-1][1]
        
        # Domain and Location
        t.domain = requ_headers[-1][0]
        t.location = "http"
        if int(requ_headers[-1][2]) == 443: t.location += "s"
        t.location += "://"+t.domain

    def __str__(t): return spacer(t.location+t.path,55) + spacer(str(t.code),4) + spacer(str(t.length),7) + spacer(t.title,30) + spacer(t.ip,15) + spacer(t.extra["payload"],12)+spacer(t.extracted[0],32)+"|"
        
    def store(t):
        
        path = t.location.split("://")[1]
        
        path = list(re.findall(r"^(.+?)(\?.+)?$", path)[0])
        if path[0][-1] != "/": path[0] += "/"
        if len(path[1]): path[1]  = urllib.parse.quote(path[1])+"_"
        path[1] += datetime.datetime.now().strftime('%y%m%d_%H%M%S')+"_"+str(random.randint(10000000, 99999999))
        path = "crawls/"+temp_loc+"/"+"".join(path)+".html"
        
        directory = os.path.dirname(path)
        if not os.path.exists(directory): os.makedirs(directory)
            
        to_write = "<!--"
        to_write += "\r\n=== REQUEST HEADER =======================================\r\n"
        to_write += t.requ_header
        to_write += "\r\n=== RESPONSE HEADER =======================================\r\n"
        to_write += t.resp_header
        to_write += "\r\n-->\r\n"
        to_write += t.body
        
        with open(path, 'w', encoding='utf-8') as file: file.write(to_write)   
               
        return path
    
class HTTP_Request:
    
    # Request data
    useragent_str = ""
    referer_str = ""
    url_str = ""
    post_str = ""
    cookies_str = ""
    flags = ""
    
    # Rules
    follow = 0
    referer_auto = False
    MAX_REQUEST = 25
    rules = []
    
    # Private data
    url_location = ""
    url_path = ""
    
    # ???
    from_list_result = []

    
    def __init__( t, url = "", post = "", flags = "", useragent = DEFAULT_USERAGENT ):
        
        if not len(url): return
        
        t.post_str = post
        t.flags = flags
        t.useragent_str = useragent
        
        t.set_url(url)

    def set_unknow_page(t):
        
        temp_url = t.url_str
        t.url_str = (t.url_location+"/..1..")
        x = t.query(RAMDOM_VALUE)
        t.default_ultra_random = [x.code, x.length, x.title]
        t.url_str = temp_url

    def set_url(t, url):
        
        t.url_str = url
        
        # Parse URL
        urlparsed = urlparse(url)
        t.url_location = urlparsed.scheme+"://"+urlparsed.netloc
        t.url_path = urlparsed.path 
        t.url_path += ("?"+urlparsed.query if urlparsed.query else "")
        t.url_path += ("#"+urlparsed.fragment if urlparsed.fragment else "")

        print("\nTARGET: "+payloadify(urlparse(url).netloc, "[PAYLOAD]")+"\n")
  
    def get_cookies(t):

        r = t.query("GET SESSIONS COOKIES")
        for c in r.cookies:
            x = re.findall(r"(\w+)=(.+)",c)
            if x:
                out = x[0][0]+"="+x[0][1]
                if re.search("(ASP|PHP).?SESSION",x[0][0],re.I): 
                    if len(requ.cookies_str) and requ.cookies_str[-1].strip() != ";": requ.cookies_str += ";"
                    requ.cookies_str += out+";"
                
        
    query_count = 0
    MAX_CHANGED = False
    callback = lambda x,t,r: print("oops")
        
    def query(t, payloads):
        
        start_time = datetime.now()
        
        command = "curl -v -s -sS"
        
        if t.follow: 
            command += " -L --max-redirs "+str(t.follow)
            if len(t.post_str): command += " --request POST -X GET"

        if len(t.useragent_str): command += ' -H "User-Agent:'+payloadify(t.useragent_str,payloads)+'"'
        

        if len(t.cookies_str): 
            command += " -b "+t.cookies_str
        if len(t.flags): command += " "+t.flags
        
        url = payloadify(t.url_str,payloads)
    
        if t.referer_auto: command += ' -e "'+url+'"'
        else:
            if len(t.referer_str): command += ' -e "'+payloadify(t.referer_str,payloads)+'"'
        
        command += " \""+ url+ "\""
    
        if len(t.post_str): 
            
            command += ' -d "'+payloadify(t.post_str,payloads)+'"'
        
            # for big POST datas use   following:
            # with open(path_to_logs+"temp_data.txt", 'w') as file: file.write(payloadify(t.post_str,payloads))  
            # command += ' -X POST -d @'+path_to_logs+"temp_data.txt"
        
        if DEBUG: print(command) 
        results = subprocess.run("echo " + json.dumps({ "count":t.query_count, "payload":urllib.parse.unquote(str(payloads)) })+" && " + command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)
        # print("DEBUG"); exit(results)
        
        t.query_count+=1
    
        # beyond 5s response time: divide max reqs
        if ((datetime.now() - start_time).total_seconds()>5 and t.MAX_CHANGED): 
            t.MAX_CHANGED = False
            # t.MAX_REQUEST = math.ceil(t.MAX_REQUEST*.5)
            # print("MAX_REQUEST: "+str(t.MAX_REQUEST))
        
        with t.print_lock:

            r = HTTP_Packet(results)
            if r.code: # could be removed ?
                
                if not r.code in t.rules:
                    c = 0
                    c += t.default_ultra_random[0] == r.code
                    c += t.default_ultra_random[1] == r.length
                    c += t.default_ultra_random[2] == r.title
                    if c < 2:
                        t.from_list_result.append(r)
                        t.callback(t,r)
            return r
            
    threads = [] 
    pending = []

    def on_key_pressed(t, event): 
        
        print (t.query_count)
        return
        
        if len(t.payloads) and event.scan_code == 44: print("POSITION: "+" : ".join(t.payloads)+ "")
        # print("keycode:"+str(event.scan_code))
    
    print_lock = threading.Lock()
    
    default_ultra_random = [0,0,0]
    
    
    def from_list(t, payloads):

        print("+---------------------------------------------------------+------+---------+--------------------------------+-----------------+--------------+----------------------------------+")
        print(spacer("URL",55) + spacer("CODE",4) + spacer("LENGTH",7) + spacer("TITLE",30) + spacer("IP",15) + spacer("PAYLOAD",12)+spacer("EXTRACT",32)+"|")
        print("+---------------------------------------------------------+------+---------+--------------------------------+-----------------+--------------+----------------------------------+")
     
        t.from_list_result = []
      
        keyboard.on_press(t.on_key_pressed)
        
        if type(payloads) != list: payloads = [payloads]
        payloads = list(set([str(item).strip() for item in payloads]))
        
        t.threads = []
        for i in range(len(payloads)): 
            
            thread = threading.Thread(target=t.query, args=([payloads[i]]))
            thread.start()
            t.threads.append(thread)  
            
            # remove thread from threads when http_req came back
    
            if i%t.MAX_REQUEST == t.MAX_REQUEST-1 or (len(t.pending)-i<t.MAX_REQUEST and i%t.MAX_REQUEST == (len(t.pending)%t.MAX_REQUEST-1)):    
                
                try: 
                    t.MAX_CHANGED = True
                    for thread in t.threads: thread.join()
                    
                except KeyboardInterrupt: os._exit(1)
                
                t.threads.clear()   
                
        return t.from_list_result    
            
    def from_file(t, file):
        
        with open(file, 'rb') as file2: enc = chardet.detect(file2.read())["encoding"]

        with open(file, 'r', encoding=enc) as file: 
            
            for line in file: t.pending.append(str(line.strip()))

        return  t.from_list(t.pending)
    

def parse_urls(o,domain=""):
    
        parsed_urls= []
        
        # Parse Urls
        for x in re.findall(r"(([\w\s_\-]+\.(php|aspx|asp|cgi)([\?\#][^(\s|<|'|\"|>)]+)?))",o,re.I|re.DOTALL): 
            parsed_urls.append(x[1].strip())
        for x in re.findall(r"<\s*form[^\>]+action=['\"]?([^(\s|'|\")]+)[^>]+>(.+?)<\s*/\s*form\s*[^>]*>",o,re.I|re.DOTALL): 
            parsed_urls.append(x[0].strip()+" POST:"+"=xx&".join(re.findall(r"name=[\"']?([^(\s|\"|')]+)",x[1]))+"=xx")
        for x in re.findall(r"(href|link|src)=['\"]?([^(\s|'|\")]+)",o,re.I|re.DOTALL): 
             if not re.findall("^\w+:",x[1]): parsed_urls.append(x[1])
        
        p = []
        
        for i in range(len(parsed_urls)): 
      
            url = parsed_urls[i]
            
            url = re.sub(r"^\./","/",url)
            
            if not re.match(r"^(https)?://",url): 
                
                if not re.match(r"^/",url): url = "/"+url
                
                url = domain+url
 
            # Remove duplicates
            if url in p: continue
            
            # Remove some useless
            if re.search(r"wp.content.uploads|xmlrpc|Apache",url): continue
        
            # Remove useless extensions
            extension = os.path.splitext(url)[1][1:].lower().replace(r"\d","")
            if extension in ["js","html", "htm", "woff","woff2","ico","css", "png", "jpg", "jpeg", "gif", "svg", "ttf", "webp", "pdf", "dtd", "webmanifest", "pro"]: continue
            
            p.append(url)
        return sorted(set((p)))



requ = HTTP_Request()

do_parse_urls = False
reg_extract = ""
def default_callback(t,r):

    extracted=[""]
    if len(reg_extract):
        reg=re.findall(reg_extract,r.body)
        if len(reg): extracted = reg
                    
    print(
        spacer(r.location+r.path,55) + 
        spacer(str(r.code),4) + 
        spacer(str(r.length),7) + 
        spacer(r.title,30) + 
        spacer(r.ip,15) + 
        spacer(r.extra["payload"],12) + 
        spacer(extracted[0],32) + 
    "|")
    
    if do_parse_urls:
        parsed_urls = parse_urls(r.body,r.location)
        for url in  parsed_urls:
            print(spacer("   "+(url)[-173:],173)+"|")
    

    return    


# TO DO : 
# retourner from_list avec un tableau
     
if __name__ == '__main__':
                   
    import argparse
    parser = argparse.ArgumentParser()

    parser.add_argument('url', help='target URL')
    parser.add_argument('-d', '--data', help='POST data')
    parser.add_argument('-c', '--cookies', help='Cookies')
    parser.add_argument('-f', '--follow', help='follow')
    parser.add_argument('-i', '--inputfile', help='input file')
    parser.add_argument('-p', '--payload', help='Paylaod')
    parser.add_argument('-o', '--output', help='Output File')
    parser.add_argument('-e', '--extract', help='Regex to extract Data')
    parser.add_argument('-4xx', '--no4xx', action='store_true', help='No 4xx')
    parser.add_argument('-D', '--discover', action='store_true', help='Auto Mode')
    parser.add_argument('--debug', action='store_true', help='Print out CURL CMD')
    parser.add_argument('--parse', action='store_true', help='Parse URLS')
    parser.add_argument('--src', action='store_true', help='show src')
    parser.add_argument('--sess', action='store_true', help='get session')

    args = parser.parse_args()
    
    if args.debug: DEBUG = True

    requ.set_url(args.url)
    
    requ.callback = default_callback
    
    if args.cookies: requ.cookies_str = args.cookies
    if args.sess: requ.get_cookies()
    
    if args.follow: requ.follow = int(args.follow)
    
    # if args.output: requ.tofile(args.output)
    if args.data: requ.post_str = args.data
    
    if args.extract: reg_extract = args.extract
        
    
    
    if args.parse: do_parse_urls = True
    
    if args.discover:  
        
        requ.set_unknow_page()
        
        do_parse_urls = True
        
        requ.set_url(requ.url_location+"/..1..")
        
        # Parse for sql warnings
        requ.extract_reg = "(.+(sql|SQL)[^(\w|\d)].+)"
        
        # # Get robots.txt
        print(spacer("# ROBOTS.TXT ",173)+"|\n"+spacer("",173)+"|")
        robots_txt = requ.query("robots.txt")

        # if robots_txt.code == 200: print("\n"+robots_txt.body+"\n")
        
        # Find techs
        print(spacer("# DISCOVERY",173)+"|\n"+spacer("",173)+"|")
        x = requ.from_list(DISCOVERY_LIST)
        print(len(x))
        print("FINIIIIISH")
        
        # # fuzz
        # print(spacer("# FUZZ",173)+"|\n"+spacer("",173)+"|")
        # requ.set_url(requ.url_location+"/..1../")
        # requ.from_file("../../lists/fuzz.txt")
        
        # requ.url_str = requ.url_location+"/..1...php"
        # requ.from_file("../../lists/fuzz.txt")
        
        # 

        
    if args.no4xx: requ.rules = (range(400,410  ))
    
    if not args.payload: args.payload = "" 
    
    
    if args.inputfile: 
        requ.set_unknow_page()
        exit(requ.from_file(args.inputfile)) 
    
    x  = requ.query(args.payload)
    

    print("+---------------------------------------------------------+------+---------+--------------------------------+-----------------+--------------+----------------------------------+")
    if args.src: 
        print()
        print(x.requ_header)
        print()
        print(x.resp_header)
        print()
        print(x.body)
    print("\n")
    