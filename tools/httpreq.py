import subprocess,re, keyboard, threading, urllib.request,os, chardet, datetime, random, ast,json,math

from datetime import datetime
from urllib.parse import urlparse
path_to_logs = "C:/Users/tourkit/Desktop/netty/logs/"

temp_loc = "kobeyes/"

def urlencode(string):
    return urllib.parse.quote(string)

def get_domain(url):
    return ".".join(urlparse(url).netloc.split(".")[-2:])

def cell(string,width=20):
    string = string[:width]
    for i in range(width+1-len(string)): string+=" "
    return "| "+string

class HTTP_Response:
   
    body = ""
    req_header = ""
    header = ""
    ip = ""
    location = ""
    payload = []
    code = 0
    length = 0
    extract = []
    title = ""
    extra = {}
    
    def __init__(t,results):   
        
        
        body = results.stdout
        encoding = chardet.detect(body)['encoding']
        if  encoding: 
            try: t.body = body.decode(encoding)
            except UnicodeDecodeError: t.body = body.decode('utf-8', 'ignore')

        encoding = chardet.detect(body)['encoding']
        body = body.decode(encoding).splitlines()
        t.extra = json.loads(body[0])
        
        t.body = "\n".join(list(body[1:]))
        
        rerr = results.stderr.decode(chardet.detect(results.stderr)["encoding"])
        rerr = re.sub(r"^[^\*]\s","",rerr,flags=re.M)
        
        if re.search(r"Could not resolve", rerr): 
            t.req_header = t.extra["original_domain"]
            t.header = ""
            return 
        
        failed = re.search(r"curl: \(([^\)]+)\) (.+)\n", rerr)
        if failed: return #print("eroooooor")

        
        header = re.findall(r'(HTTP/\d+\.\d+\s\d{3}.+?)\r\s*\r\s*\r', rerr,flags=re.DOTALL)
        t.header = header[-1]
        # print(header)
        code = re.search(r"HTTP[^\s]+\s+(\d{3})", t.header)
        if code: t.code = int(code.group(1))
        # else: print("\r\n=======================================\r\n"+rerr+"\r\n=======================================\r\n")
        length = re.search(re.compile(r"Content.Length. (\d+)", re.DOTALL), t.header)
        if length: t.length = int(length.group(1))
        else: t.length = len(t.body)  

        body_title = re.search(r"title\s*>\s*([^<]+)\s*<",t.body, re.I)
        if body_title: t.title = re.sub(r"\s+"," ",body_title[1])
        
        req_headers = re.findall(r"\* Connected to ([^\s]+) \(([^\)]+)\) port (\d+).+?\n(.+?)\s+\n\s*\n",rerr,flags=re.DOTALL)
        if not req_headers: return  print(rerr)
        t.req_header = req_headers[-1][3]
        t.ip = req_headers[-1][1]
        
        t.location = "http"
        if int(req_headers[-1][2]) == 443: t.location += "s"
        t.location += "://"+req_headers[-1][0]+"/"

    def __str__(t): return cell(t.location,45)+cell(str(t.code),3)+cell(str(t.length),7)+cell(t.title,45)+cell(t.ip,15)+cell(t.extra["payload"],30)+"|"
        
    def store(t):
        
        path = t.location.split("://")[1]
        
        path = list(re.findall(r"^(.+?)(\?.+)?$", path)[0])
        if path[0][-1] != "/": path[0] += "/"
        if len(path[1]): path[1]  = urllib.parse.quote(path[1])+"_"
        path[1] += datetime.datetime.now().strftime('%y%m%d_%H%M%S')+"_"+str(random.randint(10000000, 99999999))
        path = "crawls/"+temp_loc+"".join(path)+".html"
        
        directory = os.path.dirname(path)
        if not os.path.exists(directory): os.makedirs(directory)
            
        to_write = ""
        to_write = "<!--"
        to_write += "\r\n=== REQUEST HEADER =======================================\r\n"
        to_write += t.req_header
        to_write += "\r\n=== RESPONSE HEADER =======================================\r\n"
        to_write += t.header
        to_write += "\r\n-->\r\n"
        to_write += t.body
        
        with open(path, 'w', encoding='utf-8') as file: file.write(to_write)   
               
        return path
        

class HTTP_Packet:
    
    
    
    header = ""
    body   = ""
class HTTP_Request:
    
    ## QUERY BUILDER 
    
    reg_pattern = ""
    followredir = ""
    flags_list = []
    useragent = []
    referer = []
    domain = []
    path = []
    get_data = []
    post_data = []
    auto_referer = True
    query_count = 0
    
    rules = []
  
    MAX_REQUEST = 25
    MAX_CHANGED = False
    domain_only = ""
    parsed_urls = []
       
    callback = lambda t,r: print(r)
    payloads = []
    
    def __init__(t, url = "", post="", flags_list="-m 5" ):
        
        if len(url): t.url(url)
        if len(post): t.post(post)
        if len(flags_list): t.flags(flags_list)
 
        t.ua('Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36')

        print("TARGET: "+t.payload_join(t.domain, "[PAYLOAD]"))

    def query_split(t, q):
        parts = re.split(r'\.\..+?\.\.', q)
        separators = re.findall(r'\.\.(.+?)\.\.', q)
        result = []
        for i, part in enumerate(parts):
            result.append(part)
            if i < len(separators):
                result.append(separators[i])
        return result

    def url(t, url):
            
        results = re.findall(r"^((\.\.([^(\.\.)]+)\.\.)?http.?.//[^/]+)?(/?((.*)/)*(.*))?$",url, re.I)[0]
        if len(results)>0: t.domain = t.query_split(results[0]+"/")
        if len(results)>4: t.path =  t.query_split(results[4])
        if len(results)>6: t.get(results[6])
        t.domain_only = get_domain(results[0])
    
    def get(t, data): t.get_data =  t.query_split(data)     

    def post(t, data): t.post_data =  t.query_split(data) 
    
    def ua(t, data): t.useragent =  t.query_split(data) 
    
    def ref(t, data): t.referer =  t.query_split(data) 
    
    def flags(t, data): t.flags_list =  t.query_split(data) 

    def follow(t, maxredir=1):
       if maxredir: t.followredir = "  -L --max-redirs "+str(maxredir)
       else: t.followredir =""
    
    def extract(t, pattern): 
        t.reg_pattern = pattern
        t.callback = lambda r: print("\n".join(r.extract))
        
    def payload_join(t, string, payload):
        
        output = ""
        string_len = len(string)
        if not string_len: return False
        if len(string[0]): output+=string[0]
        
        if string_len>1:
            
            if isinstance(payload, str): payload = [payload]
                    
            for i in range(int((string_len-1)/2)):
                i2 = i*2+1; name = string[i2]; i2+=1

                if isinstance(payload, list): 
                    name = min(int(name),len(name)-1) if name.isdigit() else 0
                    output += payload[name]
                    output +=  string[i2]
                    continue
            
                if name in payload: output += payload[name] 
                else: output += ".."+name+".."
                
                output += string[i2]
                    
        return output
    
    print_lock = threading.Lock()
    
    def query(t, payload):
        
        t.payloads = [payload]
        start_time = datetime.now()
        
        t.query_count+=1
        
        # print("DEBUG PAYLOAD: " + str(payload)) 
        
        domain = t.payload_join(t.domain,payload)
        
        command = "curl -i --retry 1 --retry-connrefused -v -s -sS" + t.followredir
        
        if len(t.flags_list): command += ' '+t.payload_join(t.flags_list,payload)
        
        if len(t.post_data): 
            
            command += ' -X POST -d "'+t.payload_join(t.post_data,payload)+'"'
        
            # for big POST datas use   following:
            # with open(path_to_logs+"temp_data.txt", 'w') as file: file.write(t.payload_join(t.post_data,payload))  
            # command += ' -X POST -d @'+path_to_logs+"temp_data.txt"

        if len(t.useragent): command += ' -H "User-Agent:'+t.payload_join(t.useragent,payload)+'"'
        
        if t.auto_referer: command += ' -e "'+domain+'"'
        else:
            if len(t.referer): command += ' -e "'+t.payload_join(t.referer,payload)+'"'
        full_domain = ""   
        full_domain += domain
        full_domain += t.payload_join(t.path,payload)
        full_domain += t.payload_join(t.get_data,payload)
        
        command += ' "'
        command += full_domain
        command += '"'
        
        data = {
            
            "count":t.query_count, 
            "payload":urllib.parse.unquote(str(payload)),
            "original_domain":full_domain
            
        }
        
        command = "echo "+json.dumps(data)+" && "+command
        
        # print("DEBUG COMMAND: " + str(command)) 
        results = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)
        # print("DEBUG"); exit(results)
    
        end_time = datetime.now()
        time_difference = (end_time - start_time).total_seconds()
        
        # beyond 5s response time: divide max reqs
        if (time_difference>5 and t.MAX_CHANGED): 
            t.MAX_CHANGED = False
            # t.MAX_REQUEST = math.ceil(t.MAX_REQUEST*.5)
            # print("MAX_REQUEST: "+str(t.MAX_REQUEST))
        
        with t.print_lock:
            
            r = HTTP_Response(results)
            if r.code: 
                if not r.code in t.rules:
                    t.callback(r)
            return 
  
    threads = [] 
    pending = []

    def on_key_pressed(t, event): 
        # return
        
        if len(t.payloads) and event.scan_code == 44: print("POSITION: "+" : ".join(t.payloads)+ "")
        # print("keycode:"+str(event.scan_code))

    def add_thread(t, payload):
        
        thread = threading.Thread(target=t.query, args=([payload]))
        thread.start()
        t.threads.append(thread)  
    
        # remove thread from threads when http_req came back
    
    def from_list(t, lst):
        
        lst = list(set([str(item).strip() for item in lst]))
        
        i = 0
        t.threads = []
        for p in lst: 
            
            t.add_thread(p)
            
            if i%t.MAX_REQUEST == t.MAX_REQUEST-1 or (len(t.pending)-i<t.MAX_REQUEST and i%t.MAX_REQUEST == (len(t.pending)%t.MAX_REQUEST-1)):    
                
                try: 
                    t.MAX_CHANGED = True
                    for thread in t.threads: thread.join()
                    for thread in t.threads: thread.join()
                    
                except KeyboardInterrupt: os._exit(1)
                
                t.threads.clear()
                
            i+=1
        
    def from_file(t, file):
      
        keyboard.on_press(t.on_key_pressed)
        i = 0
        
        with open(file, 'rb') as file2:
            enc = chardet.detect(file2.read())["encoding"]

        with open(file, 'r', encoding=enc) as file: 
            for line in file: 
                line = line.strip()
                t.pending.append(str(line))
                i+=1
        # return        
        t.from_list(t.pending)

if __name__ == '__main__':
                   
    import argparse
    parser = argparse.ArgumentParser()

    parser.add_argument('url', help='target URL')
    parser.add_argument('-d', '--data', help='POST data')
    parser.add_argument('-f', '--follow', help='follow')
    parser.add_argument('-i', '--inputfile', help='input file')
    parser.add_argument('-p', '--payload', help='Paylaod')
    parser.add_argument('-o', '--output', help='Output File')
    parser.add_argument('-4xx', '--no4xx', action='store_true', help='No 4xx')


    args = parser.parse_args()

    
    url = args.url
    payload = args.payload
    output = args.output

    t_url = url

    r = HTTP_Request(url)

    if args.follow: r.follow(args.follow)
    if args.data: r.data(args.data)

    if not args.payload: args.payload = "" 

    # if args.output: r.tofile(args.output)
    if args.no4xx: r.rules = (range(400,410))
    
    if args.inputfile:
        r.from_file(args.inputfile) 
    else:
        r.query(args.payload)
