import re,  urllib.request,os, chardet, datetime, random, json


class HTTP_Packet:
   
    body = ""
    requ_header = ""
    resp_header = ""
    code = 0
    length = 0
    cookies = ""
    ip = ""
    domain = ""
    location = ""
    path=""
    title = ""
    extra = {}
    
    def __init__(t,results):   
        
        # Read STDOUT
        body = results.stdout
        encoding = chardet.detect(body)['encoding']
        if  encoding: 
            try: body = body.decode(encoding)
            except UnicodeDecodeError: body = body.decode('utf-8', 'ignore')

        body = body.splitlines()
        t.extra = json.loads(body[0])
        t.body = "\n".join(list(body[1:]))
        # print(t.body)
        
        # Read STDERR and check for errors
        rerr = re.sub(r"^[^\*]\s","",results.stderr.decode(chardet.detect(results.stderr)["encoding"]),flags=re.M)
        # print(rerr)
        if re.search(r"Could not resolve", rerr): return 
        failed = re.findall(r"curl: \([^\)]+\) .+\n", rerr)
        if failed: return #print(failed)

        # Parse Response Header
        resp_header = re.findall(r'(HTTP/\d+\.\d+\s\d{3}.+?)\s*\n\s*\n', rerr,flags=re.DOTALL)
        t.resp_header = resp_header[-1]
        
        # HTTP code
        code = re.search(r"HTTP[^\s]+\s+(\d{3})", t.resp_header)
        if code: t.code = int(code.group(1))
        
        # Cookies
        cookies = re.findall(r"Set.Cookies?:\s*([^\n]+)\s*", t.resp_header)
        if cookies: t.cookies = cookies[0].split(";")

        # Content lenght
        length = re.search(re.compile(r"Content.Length. (\d+)", re.DOTALL), t.resp_header)
        if length: t.length = int(length.group(1))
        else: t.length = len(t.body)  

        # Page title
        body_title = re.search(r"title\s*>\s*([^<]+)\s*<",t.body, re.I)
        if body_title: t.title = re.sub(r"\s+"," ",body_title[1])
        
        # Parse request Headers 
        requ_headers = re.findall(r"\* Connected to ([^\s]+) \(([^\)]+)\) port (\d+).+?\n(\*.+?\n)*(.+?)\s+\n\s*\n",rerr,flags=re.DOTALL)
        # for x in requ_headers[0]: print(x[:150]+"\n-----")
        # exit()
        t.requ_header = requ_headers[-1][4]

        # Path
        t.path = re.findall(fr"[^\s]+[^\s]+\s([^\s]+)\sHTTP/\d\.\d",t.requ_header,re.M)[0]
        
        # IP
        t.ip = requ_headers[-1][1]
        
        # Domain and Location
        t.domain = requ_headers[-1][0]
        t.location = "http"
        if int(requ_headers[-1][2]) == 443: t.location += "s"
        t.location += "://"+t.domain

    def __str__(t): return t.location+t.path
    def store(t, temp_loc=""):
        
        path = t.location.split("://")[1]
        
        path = list(re.findall(r"^(.+?)(\?.+)?$", path)[0])
        if path[0][-1] != "/": path[0] += "/"
        if len(path[1]): path[1]  = urllib.parse.quote(path[1])+"_"
        path[1] += datetime.datetime.now().strftime('%y%m%d_%H%M%S')+"_"+str(random.randint(10000000, 99999999))
        path = "crawls/"+temp_loc+"/"+"".join(path)+".html"
        
        directory = os.path.dirname(path)
        if not os.path.exists(directory): os.makedirs(directory)
            
        to_write = "<!--"
        to_write += "\r\n=== REQUEST HEADER =======================================\r\n"
        to_write += t.requ_header
        to_write += "\r\n=== RESPONSE HEADER =======================================\r\n"
        to_write += t.resp_header
        to_write += "\r\n-->\r\n"
        to_write += t.body
        
        with open(path, 'w', encoding='utf-8') as file: file.write(to_write)   
               
        return path